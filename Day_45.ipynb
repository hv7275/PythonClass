{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Python Series \u2013 Day 45: Web Scraping in Python using BeautifulSoup\n",
    "\n",
    "## 1. Introduction\n",
    "**Web Scraping** is the process of extracting data from websites automatically.\n",
    "\n",
    "**Uses:**\n",
    "- Price Monitoring\n",
    "- Market Research\n",
    "- News Aggregation\n",
    "\n",
    "**Libraries:**\n",
    "- `requests`: To download the HTML content of a page.\n",
    "- `BeautifulSoup` (bs4): To parse and navigate the HTML tree.\n",
    "\n",
    "**Important:** Always check a website's `robots.txt` file and terms of service before scraping. Be respectful and don't overload servers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Installing BeautifulSoup\n",
    "Run these commands in your terminal (or cell with `!`) to install the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install beautifulsoup4\n",
    "# !pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fetching a Webpage\n",
    "We use `requests.get()` to fetch the page content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://quotes.toscrape.com/\"\n",
    "response = requests.get(url)\n",
    "print(f\"Status Code: {response.status_code}\") # 200 means success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parsing HTML\n",
    "We create a BeautifulSoup object to parse the raw HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "# print(soup.prettify()) # Uncomment to see the structured HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Extracting Specific Data\n",
    "BeautifulSoup provides methods like `.find()` (first match) and `.find_all()` (list of matches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Title\n",
    "print(\"Page Title:\", soup.title.text)\n",
    "\n",
    "# Extract all links\n",
    "links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "print(f\"\\nFound {len(links)} links. First 5:\")\n",
    "print(links[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Selecting Elements Using CSS Selectors\n",
    "`.select()` allows you to use CSS-style selectors (e.g., `.class`, `#id`, `tag`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select elements with class 'text'\n",
    "quotes_text = soup.select(\".text\") \n",
    "print(\"First quote using CSS selector:\", quotes_text[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scraping Example: Quotes Website\n",
    "Target: https://quotes.toscrape.com/\n",
    "Goal: Extract Quote, Author, and Tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quote_elements = soup.find_all(\"div\", class_=\"quote\")\n",
    "\n",
    "scraped_data = []\n",
    "\n",
    "for q in quote_elements:\n",
    "    text = q.find(\"span\", class_=\"text\").text\n",
    "    author = q.find(\"small\", class_=\"author\").text\n",
    "    tags = [tag.text for tag in q.find_all(\"a\", class_=\"tag\")]\n",
    "    \n",
    "    scraped_data.append({\n",
    "        \"quote\": text,\n",
    "        \"author\": author,\n",
    "        \"tags\": tags\n",
    "    })\n",
    "\n",
    "print(f\"Scraped {len(scraped_data)} quotes.\")\n",
    "print(\"Example:\", scraped_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Creating a Scraping Function\n",
    "Encapsulating logic for reusability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quotes_from_page(url):\n",
    "    r = requests.get(url)\n",
    "    s = BeautifulSoup(r.text, \"html.parser\")\n",
    "    return [q.text for q in s.find_all(\"span\", class_=\"text\")]\n",
    "\n",
    "quotes = get_quotes_from_page(\"https://quotes.toscrape.com/\")\n",
    "for i, q in enumerate(quotes[:3], 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pagination Handling (Intro)\n",
    "Most sites split data across pages. You need to find the \"Next\" button link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page = soup.find(\"li\", class_=\"next\")\n",
    "if next_page:\n",
    "    next_url = \"https://quotes.toscrape.com\" + next_page.a[\"href\"]\n",
    "    print(\"Next Page URL:\", next_url)\n",
    "else:\n",
    "    print(\"No next page found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Writing Scraped Data to CSV/JSON\n",
    "Storing your hard-earned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "\n",
    "# Save to CSV\n",
    "with open(\"quotes.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Quote\", \"Author\", \"Tags\"])\n",
    "    for item in scraped_data:\n",
    "        writer.writerow([item['quote'], item['author'], \", \".join(item['tags'])])\n",
    "print(\"Saved to quotes.csv\")\n",
    "\n",
    "# Save to JSON\n",
    "with open(\"quotes.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(scraped_data, f, indent=4)\n",
    "print(\"Saved to quotes.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Mini Project \u2013 News Scraper\n",
    "A simple scraper for a news site (Mock logic used for demonstration if site structure changes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news_headlines():\n",
    "    # Target: BBC News (This is an example, real selectors change often)\n",
    "    url = \"https://www.bbc.com/news\"\n",
    "    try:\n",
    "        r = requests.get(url)\n",
    "        s = BeautifulSoup(r.text, \"html.parser\")\n",
    "        \n",
    "        # Identifying headlines (Generic H2/H3 search for stability in example)\n",
    "        headlines = s.find_all(['h2', 'h3'], limit=5)\n",
    "        \n",
    "        print(f\"--- Top Headlines from {url} ---\\n\")\n",
    "        for h in headlines:\n",
    "            text = h.get_text().strip()\n",
    "            if text:\n",
    "                print(f\"- {text}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error scraping news:\", e)\n",
    "\n",
    "scrape_news_headlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Practice Exercises\n",
    "1. Scrape the names of top repositories from a GitHub trending page (requires handling complex HTML).\n",
    "2. Write a script to monitor the price of a product on a test ecommerce site.\n",
    "3. Scrape weather information (Temperature, Condition) from a weather forecast site.\n",
    "4. Modify pagination logic to scrape the first 5 pages of quotes automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Day 45 Summary\n",
    "- **Requests:** Fetching web pages.\n",
    "- **BeautifulSoup:** Parsing and extracting data.\n",
    "- **Selectors:** `find`, `find_all`, `select`.\n",
    "- **Storage:** Saving data to CSV/JSON files.\n",
    "\n",
    "**Next topic: Day 46 \u2013 API Project (Advanced)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}