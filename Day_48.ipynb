{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Title: Python Series \u2013 Day 48: Advanced Pandas (Data Cleaning & Transformation)\n",
    "\n",
    "## 1. Introduction\n",
    "**Data Cleaning** is a critical step in any data project. Real-world data is often messy, containing missing values, duplicates, and inconsistent formats.\n",
    "\n",
    "**Today's Focus:**\n",
    "- Handling Missing Data (NaN)\n",
    "- Removing Duplicates\n",
    "- String Operations & Transformations\n",
    "- Grouping and Aggregation\n",
    "- merging and Concatenating DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Pandas & Load Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"Name\": [\"Ali\", \"Sara\", None, \"Hina\", \"Ali\"],\n",
    "    \"Age\": [20, None, 22, 21, 20],\n",
    "    \"Marks\": [85, 92, None, 75, 85],\n",
    "    \"City\": [\"Lahore\", \"Karachi\", \"Lahore\", None, \"Lahore\"]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Handling Missing Data (NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values Count:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Filling missing values\n",
    "# df.fillna(0) # Fills all NaNs with 0 (Be careful!)\n",
    "\n",
    "df[\"City\"].fillna(\"Unknown\", inplace=True)\n",
    "print(\"\\nAfter filling City:\")\n",
    "display(df)\n",
    "\n",
    "# Forward Fill (propagates last valid observation forward)\n",
    "# df.fillna(method=\"ffill\")\n",
    "\n",
    "# Dropping rows with ANY missing value\n",
    "# df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Handling Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Duplicates:\", df.duplicated().sum())\n",
    "\n",
    "df.drop_duplicates(inplace=True, keep=\"first\")\n",
    "print(\"\\nAfter Removing Duplicates:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Replacing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"City\"].replace({\"Lahore\": \"LHR\", \"Karachi\": \"KHI\"}, inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. String Operations\n",
    "Pandas provides string methods via `.str` accessor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Uppercase (Handling NaN in Name first to avoid errors if any left)\n",
    "df[\"Name\"] = df[\"Name\"].fillna(\"Unknown\").str.upper()\n",
    "\n",
    "# Check content\n",
    "lhr_city = df[df[\"City\"].str.contains(\"LHR\", na=False)]\n",
    "print(\"Rows with City containing 'LHR':\")\n",
    "display(lhr_city)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Applying Functions\n",
    "- `apply()`: Works on Series (columns) or DataFrames.\n",
    "- `map()`: Works on Series for substitution.\n",
    "- `applymap()`: Works element-wise on DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase marks by 10% using lambda\n",
    "df[\"Marks\"] = df[\"Marks\"].apply(lambda x: x * 1.1 if pd.notnull(x) else x)\n",
    "\n",
    "# Map City names back to full form\n",
    "df[\"City\"] = df[\"City\"].map({\"LHR\": \"Lahore\", \"KHI\": \"Karachi\", \"Unknown\": \"Unknown\"})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Binning / Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Age Groups\n",
    "# Note: Filling NaN Age for this example\n",
    "df[\"Age\"] = df[\"Age\"].fillna(df[\"Age\"].mean())\n",
    "\n",
    "df[\"Age_Group\"] = pd.cut(df[\"Age\"], bins=[0, 20, 30, 100], labels=[\"Junior\", \"Young Adult\", \"Senior\"])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. GroupBy (Very Important)\n",
    "Split-Apply-Combine strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Average Marks by City:\")\n",
    "print(df.groupby(\"City\")[\"Marks\"].mean())\n",
    "\n",
    "print(\"\\nDetailed Aggregation:\")\n",
    "print(df.groupby(\"City\").agg({\"Marks\": \"mean\", \"Age\": \"max\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Merging & Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({\"ID\": [1,2], \"Name\": [\"Ali\", \"Sara\"]})\n",
    "df2 = pd.DataFrame({\"ID\": [1,2], \"Score\": [88, 92]})\n",
    "\n",
    "merged = pd.merge(df1, df2, on=\"ID\")\n",
    "print(\"Merged Data:\")\n",
    "display(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], axis=1) # Side by side\n",
    "print(\"Concatenated Data:\")\n",
    "display(df_concat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Sorting & Reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(\"Marks\", ascending=False, inplace=True)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Practice Exercises\n",
    "1. Convert all 'City' names to Uppercase in the original `df`.\n",
    "2. Remove duplicate rows based on specific columns subset.\n",
    "3. Fill missing 'Marks' with the class average.\n",
    "4. Group by 'Age_Group' and find the count of students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Mini Project \u2013 Data Cleaning Pipeline\n",
    "We will simulate a raw messy dataset and clean it sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create Mock Raw Data\n",
    "raw_data = {\n",
    "    \"STUDENT NAME\": [\"  Ali  \", \"Sara\", \"ALI\", \"zara\", None],\n",
    "    \" marKS \": [80, 95, 80, None, 40],\n",
    "    \"Grade\": [None, \"A\", None, \"F\", \"F\"]\n",
    "}\n",
    "df_raw = pd.DataFrame(raw_data)\n",
    "df_raw.to_csv(\"students_raw.csv\", index=False)\n",
    "print(\"Created students_raw.csv\")\n",
    "\n",
    "# 2. Pipeline Implementation\n",
    "def clean_data_pipeline(filename):\n",
    "    # Load\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Standardize Headers\n",
    "    df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Clean Strings\n",
    "    df['student_name'] = df['student_name'].str.strip().str.title()\n",
    "    df.dropna(subset=['student_name'], inplace=True)\n",
    "    \n",
    "    # Handle Missing Numeric\n",
    "    df['marks'] = df['marks'].fillna(df['marks'].mean())\n",
    "    \n",
    "    # Create New Column\n",
    "    df['status'] = df['marks'].apply(lambda x: \"Pass\" if x >= 50 else \"Fail\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Run Pipeline\n",
    "cleaned_df = clean_data_pipeline(\"students_raw.csv\")\n",
    "print(\"\\n--- Cleaned Data ---\")\n",
    "display(cleaned_df)\n",
    "\n",
    "# Save\n",
    "cleaned_df.to_csv(\"students_cleaned.csv\", index=False)\n",
    "print(\"Saved to students_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Day 48 Summary\n",
    "- Data Cleaning is a prerequisite for analysis.\n",
    "- Pandas offers robust tools for handling Nulls, Duplicates, and Strings.\n",
    "- `groupby` and `merge` are essential for complex data manipulation.\n",
    "\n",
    "**Next topic: Day 49 \u2013 Data Visualization with Matplotlib**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}